{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from typing import Callable\n",
    "import imageio\n",
    "from pathlib import Path\n",
    "import timeit as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the planar flow class chains the planar flow layers using nn.Sequential\n",
    "\n",
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, K: int = 6): #K is the number of planar flow layers chained together\n",
    "        super().__init__()\n",
    "        \n",
    "        #chain the planar transforms together in a list\n",
    "        self.layers = [PlanarTransform() for _ in range(K)]\n",
    "        \n",
    "        #create the model from the list\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, z: Tensor) -> Tuple[Tensor, float]:\n",
    "        \n",
    "        #set the log of the Jacobian to zero\n",
    "        log_det_J = 0\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            #sum the log Jacobian of each layer\n",
    "            log_det_J += layer.log_det_J(z)\n",
    "            \n",
    "            #calculate the new points from the planar flow\n",
    "            z = layer(z)\n",
    "            \n",
    "        return z, log_det_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the planar transform class contains the functions for the planar flow\n",
    "\n",
    "class PlanarTransform(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #randomly assign starting values to the planar flow parameters from a normal distribution\n",
    "        self.u = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
    "        self.w = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
    "        self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
    "        \n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #check the invertibility condition\n",
    "        if self.u*self.w < -1:\n",
    "            \n",
    "            #update u to ensure invertibility\n",
    "            self.get_u_hat()\n",
    "        \n",
    "        #return the planar flow layer function\n",
    "        return z + self.u*nn.Tanh()(z*self.w + self.b)\n",
    "    \n",
    "    def log_det_J(self, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #check the invertibility condition\n",
    "        if self.u*self.w < -1:\n",
    "            self.get_u_hat()\n",
    "            \n",
    "        #calculate the log of the Jacobian\n",
    "        a = z*self.w + self.b\n",
    "        psi = (1 - nn.Tanh()(a)**2)*self.w\n",
    "        abs_det = (1 + self.u*psi).abs()\n",
    "        log_det = torch.log(1e-10 + abs_det)\n",
    "        \n",
    "        return log_det\n",
    "    \n",
    "    def get_u_hat(self) -> None:\n",
    "        \n",
    "        #invertibility condition\n",
    "        wtu = self.u*self.w\n",
    "        m_wtu = -1 + torch.log(1 + torch.exp(wtu))\n",
    "        self.u.data = (self.u + (m_wtu - wtu)*self.w/self.w**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the target distribution class holds the definitions for the 1D target distributions used in the examples\n",
    "\n",
    "class TargetDistribution:\n",
    "    \n",
    "    def __init__(self, name: str, t: int, a: int = 1): #t is the annealing value, a is the mean for symmetric/assymetric GMM\n",
    "        \n",
    "        #get the name of the target distribution to be used\n",
    "        self.func = self.get_target_distribution(name, t, a)\n",
    "        \n",
    "    def __call__(self, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #return the target function\n",
    "        return self.func(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_target_distribution(name: str, t: int, a) -> Callable[[Tensor], Tensor]:\n",
    "        \n",
    "        if name == \"MotivatingBimodal\":\n",
    "            \n",
    "            def MotivatingBimodal(z):\n",
    "                f = 0.953973*torch.exp(-t*((z + 2)**2 - 3)**2)\n",
    "                return f\n",
    "            \n",
    "            return MotivatingBimodal\n",
    "        \n",
    "        if name == \"SymmetricBimodal\":\n",
    "            def SymmetricBimodal(z):\n",
    "                f = 0.5*1/np.sqrt(np.pi/8)*torch.exp(-8*t*(z + a)**2) + 0.5*1/np.sqrt(np.pi/8)*torch.exp(-8*t*(z - a)**2)\n",
    "                return f\n",
    "                \n",
    "            return SymmetricBimodal\n",
    "        \n",
    "        if name == \"AsymmetricBimodal\":\n",
    "            def AsymmetricBimodal(z):\n",
    "                f = 0.5*1/np.sqrt(np.pi/8)*torch.exp(-8*t*(z + a)**2) + 0.5*1/np.sqrt(np.pi/8)*torch.exp(-8*t*(z)**2)\n",
    "                return f\n",
    "                \n",
    "            return AsymmetricBimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free energy loss function used\n",
    "\n",
    "class VariationalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, distribution: TargetDistribution, mean: int, std: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        #the target distribution\n",
    "        self.distr = distribution\n",
    "        \n",
    "        #the starting distribution\n",
    "        self.base_distr = Normal(mean, std)\n",
    "        \n",
    "    def forward(self, z0: Tensor, z: Tensor, sum_log_det_J: float) -> float:\n",
    "        \n",
    "        #calculate the log of the starting distribution at initial points z0\n",
    "        base_log_prob = self.base_distr.log_prob(z0)\n",
    "        \n",
    "        #calculate the log of the target distribution at final points z\n",
    "        target_density_log_prob = torch.log(self.distr(z) + 1e-10)\n",
    "        \n",
    "        #calculate the free energy\n",
    "        return (base_log_prob - target_density_log_prob - sum_log_det_J).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definitions required for plotting purposes\n",
    "\n",
    "#the starting function\n",
    "def StartingFunction(x, mean, std):\n",
    "    return 1/np.sqrt(2*(std**2)*np.pi)*np.exp(-(x+mean)**2/(2*std**2))\n",
    "\n",
    "#the jacobian calculation using tanh\n",
    "def Jacobian(x, P):\n",
    "    return np.abs(1 + P[0]*(1-np.tanh(P[1]*x + P[2])**2)*P[1])\n",
    "\n",
    "#compute the inverse\n",
    "def H(x, P):\n",
    "    return x + P[0]*np.tanh(P[1]*x + P[2])\n",
    "\n",
    "def computeInverse(z, P):\n",
    "    hInverse = (z - P[0]*P[2])/(1 + P[0]*P[1])\n",
    "    \n",
    "    for j in range(len(hInverse)):\n",
    "        \n",
    "        def optFun(x):\n",
    "            return H(x, P) - z[j]\n",
    "        \n",
    "        hInverse[j] = scipy.optimize.fsolve(optFun, hInverse[j])\n",
    "        \n",
    "    return hInverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the definitions for plotting\n",
    "\n",
    "plt.rc('font', family='Arial') \n",
    "plt.rc('xtick', labelsize='x-small') \n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "    \n",
    "def plot_density(density, xlim=5):\n",
    "        \n",
    "    #plotting points\n",
    "    x = np.linspace(-xlim, xlim, 1000)\n",
    "    \n",
    "    #calculate the density function\n",
    "    y = density(torch.tensor(x))\n",
    "        \n",
    "    #plot\n",
    "    fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "    plt.plot(x, y, 'darkmagenta')\n",
    "    plt.title('Target PDF')\n",
    "    #plt.savefig(f'Target PDF.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def compute_opt(mean, std, xlim):\n",
    "    \n",
    "    #put the parameters into a vector\n",
    "    P = []\n",
    "    for param in model.parameters():\n",
    "        p = param.detach().numpy().tolist()\n",
    "        p = np.reshape(p, -1)\n",
    "        P = np.concatenate([P, p])\n",
    "            \n",
    "    #points\n",
    "    x = np.linspace(-xlim, xlim, 1000)\n",
    "    optimized_pdf = np.ones(len(x))\n",
    "    \n",
    "    xy = x\n",
    "    i = 3*(flow_length - 1)\n",
    "    for j in range(flow_length):\n",
    "        xy = computeInverse(xy, P[i:i+3])\n",
    "        optimized_pdf = optimized_pdf/Jacobian(xy, P[i:i+3])\n",
    "        i-=3\n",
    "            \n",
    "    optimized_pdf = optimized_pdf*StartingFunction(xy, mean, std)\n",
    "    \n",
    "    return x, optimized_pdf\n",
    "    \n",
    "def plot_optimized(mean, std, xlim=5):\n",
    "    \n",
    "    #compute the optimized density function\n",
    "    x, optimized_pdf = compute_opt(mean, std, xlim)\n",
    "    \n",
    "    #plot\n",
    "    fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "    plt.plot(x, optimized_pdf, 'darkmagenta')\n",
    "    plt.title('Optimized PDF')\n",
    "    #plt.savefig(f'Optimized PDF.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_comparison(mean, std, xlim=5):\n",
    "    \n",
    "    #compute the optimized density function\n",
    "    x, opt = compute_opt(mean, std, xlim)\n",
    "    \n",
    "    #calculate the starting density function\n",
    "    start = StartingFunction(x, mean, std)\n",
    "    \n",
    "    #calculate the true density function\n",
    "    density_target = TargetDistribution(target_distr, t=1, a=a)\n",
    "    true = density_target(torch.tensor(x))\n",
    "    \n",
    "    #plot\n",
    "    fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "    plt.plot(x, start, 'gold', lw=1, label='Starting')\n",
    "    plt.plot(x, true, 'tomato', lw=1, label='Target')\n",
    "    plt.plot(x, opt, 'darkmagenta', lw=1, ls='--', label='Optimized')\n",
    "    plt.legend()\n",
    "    plt.title('Comparison of PDFs')\n",
    "    #plt.savefig(f'Comparison PDFs.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_tvals(tvals):\n",
    "    \n",
    "    #plot\n",
    "    fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "    plt.plot(np.linspace(1,len(tvals), len(tvals)), tvals, 'darkmagenta', lw=1)\n",
    "    plt.title('Annealing Schedule')\n",
    "    #plt.savefig(f'Annealing Schedule.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#AdaAnn scheduler set-up\n",
    "\n",
    "#list to collect t values\n",
    "tvals = []\n",
    "\n",
    "#choose the starting distribution parameters\n",
    "mean_starting = 0\n",
    "std_starting = 2\n",
    "    \n",
    "#choose the target distribution and set a (for symmetric/asymmetric)\n",
    "target_distr = 'MotivatingBimodal'\n",
    "#target_distr = 'SymmetricBimodal'\n",
    "#target_distr = 'AsymmetricBimodal'\n",
    "\n",
    "a = 1                  #a needs to be defined for MotivatingBimodal but is not used\n",
    "\n",
    "#plot target density\n",
    "density_target = TargetDistribution(target_distr, t=1, a=a)\n",
    "plot_density(density_target, 5)\n",
    "\n",
    "#set the parameters\n",
    "flow_length = 50       #number of planar flow layers K\n",
    "lr = 0.005             #learning rate for the optimizer\n",
    "t0 = 0.01              #starting t value\n",
    "tol = 0.01             #KL divergence tolerance for AdaAnn\n",
    "M = 1000               #number of sample points to compute step size\n",
    "dt = 0\n",
    "\n",
    "#set the number of samples in each batch\n",
    "N = 100                #at t0 and each annealing step\n",
    "N_1 = 1000             #at t = 1\n",
    "\n",
    "#set the number of iterations\n",
    "T_0 = 500              #at t0\n",
    "T = 2                  #at each annealing step\n",
    "T_1 = 8000             #at t = 1\n",
    "  \n",
    "#create the model and optimizer using Adam\n",
    "model = PlanarFlow(K=flow_length)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#start time\n",
    "start = time.default_timer()\n",
    "\n",
    "#optimization using AdaAnn\n",
    "t = t0\n",
    "while t < 1:\n",
    "    \n",
    "    #new t value\n",
    "    t = min(1, t + dt)\n",
    "    tvals = np.concatenate([tvals, np.array([t])])\n",
    "    \n",
    "    #number of iterations and batch size at each annealing step\n",
    "    num_iter = T\n",
    "    batch_size = N\n",
    "    \n",
    "    #update parameters at t0\n",
    "    if t == t0:\n",
    "        num_iter = T_0\n",
    "        \n",
    "    #update parameters at t = 1 and include a learning rate scheduler\n",
    "    if t == 1:\n",
    "        num_iter = T_1\n",
    "        batch_size = N_1\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "            \n",
    "    #update the target density and loss function with current t value\n",
    "    density = TargetDistribution(target_distr, t, a=a)\n",
    "    bound = VariationalLoss(density, mean=mean_starting, std=std_starting)\n",
    "        \n",
    "    #train the model  \n",
    "    for iter_num in range(1, num_iter + 1):\n",
    "            \n",
    "        #get the batches from starting distribution\n",
    "        batch = torch.zeros(batch_size).normal_(mean=mean_starting, std=std_starting)\n",
    "\n",
    "        #pass the batch through the planar flow model\n",
    "        zk, log_jacobians = model(batch)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = bound(batch, zk, log_jacobians)\n",
    "\n",
    "        #train the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        #apply a learning rate scheduler when t = 1\n",
    "        if t == 1:\n",
    "            scheduler.step()\n",
    "            \n",
    "    #compute the dt value using M points\n",
    "    density_dt = TargetDistribution(target_distr, t=1)\n",
    "    zk, log_jacobians = model(torch.zeros(M).normal_(mean=mean_starting, std=std_starting))\n",
    "    log_qk = torch.log(density_dt(zk) + 1e-10)\n",
    "    #log_qk = np.log(0.953973) - ((zk + 2)**2 - 3)**2                #could use exact log calculation for motivating\n",
    "    dt = tol/torch.sqrt(log_qk.var())\n",
    "    dt = dt.detach().numpy()\n",
    "\n",
    "#compute time\n",
    "end = time.default_timer()\n",
    "opt_time = end - start\n",
    "    \n",
    "#plot approximation and annealing schedule\n",
    "plot_comparison(mean=mean_starting, std=std_starting, lim=5)\n",
    "plot_tvals(tvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear scheduler set-up\n",
    "\n",
    "#list to collect t values\n",
    "tvals = []\n",
    "\n",
    "#choose the starting distribution parameters\n",
    "mean_starting = 0\n",
    "std_starting = 2\n",
    "    \n",
    "#choose the target distribution and set a (for symmetric/asymmetric)\n",
    "target_distr = 'MotivatingBimodal'\n",
    "#target_distr = 'SymmetricBimodal'\n",
    "#target_distr = 'AsymmetricBimodal'\n",
    "\n",
    "a = 1                  #a needs to be defined for MotivatingBimodal but is not used\n",
    "\n",
    "#plot target density\n",
    "density_target = TargetDistribution(target_distr, t=1, a=a)\n",
    "plot_density(density_target, 5)\n",
    "\n",
    "#set the parameters\n",
    "flow_length = 50       #number of planar flow layers K\n",
    "lr = 0.005             #learning rate for the optimizer\n",
    "t0 = 0.01              #starting t value\n",
    "eps = 1/10000          #set constant step size\n",
    "dt = 0\n",
    "\n",
    "#set the number of samples in each batch\n",
    "N = 100                #at t0 and each annealing step\n",
    "N_1 = 1000             #at t = 1\n",
    "\n",
    "#set the number of iterations\n",
    "T_0 = 500              #at t0\n",
    "T = 1                  #at each annealing step\n",
    "T_1 = 8000             #at t = 1\n",
    "  \n",
    "#create the model and optimizer using Adam\n",
    "model = PlanarFlow(K=flow_length)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#start time\n",
    "start = time.default_timer()\n",
    "\n",
    "#optimization using linear scheduler\n",
    "t = t0\n",
    "while t < 1:\n",
    "    \n",
    "    #new t value\n",
    "    t = min(1, t + dt)\n",
    "    tvals = np.concatenate([tvals, np.array([t])])\n",
    "    \n",
    "    #number of iterations and batch size at each annealing step\n",
    "    num_iter = T\n",
    "    batch_size = N\n",
    "    \n",
    "    #update parameters at t0\n",
    "    if t == t0:\n",
    "        num_iter = T_0\n",
    "        \n",
    "    #update parameters at t = 1 and include a learning rate scheduler\n",
    "    if t == 1:\n",
    "        num_iter = T_1\n",
    "        batch_size = N_1\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "            \n",
    "    #update the target density and loss function with current t value\n",
    "    density = TargetDistribution(target_distr, t, a=a)\n",
    "    bound = VariationalLoss(density, mean=mean_starting, std=std_starting)\n",
    "        \n",
    "    #train the model  \n",
    "    for iter_num in range(1, num_iter + 1):\n",
    "            \n",
    "        #get the batches from starting distribution\n",
    "        batch = torch.zeros(batch_size).normal_(mean=mean_starting, std=std_starting)\n",
    "\n",
    "        #pass the batch through the planar flow model\n",
    "        zk, log_jacobians = model(batch)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = bound(batch, zk, log_jacobians)\n",
    "\n",
    "        #train the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        #apply a learning rate scheduler when t = 1\n",
    "        if t == 1:\n",
    "            scheduler.step()\n",
    "                \n",
    "    dt = eps\n",
    "\n",
    "#compute time\n",
    "end = time.default_timer()\n",
    "opt_time = end - start\n",
    "    \n",
    "#plot approximation and annealing schedule\n",
    "plot_comparison(mean=mean_starting, std=std_starting, lim=5)\n",
    "plot_tvals(tvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no scheduler set-up\n",
    "\n",
    "#choose the starting distribution parameters\n",
    "mean_starting = 0\n",
    "std_starting = 2\n",
    "    \n",
    "#choose the target distribution and set a (for symmetric/asymmetric)\n",
    "target_distr = 'MotivatingBimodal'\n",
    "#target_distr = 'SymmetricBimodal'\n",
    "#target_distr = 'AsymmetricBimodal'\n",
    "\n",
    "a = 1                  #a needs to be defined for MotivatingBimodal but is not used\n",
    "\n",
    "#plot target density\n",
    "density_target = TargetDistribution(target_distr, t=1, a=a)\n",
    "plot_density(density_target, 5)\n",
    "\n",
    "#set the parameters\n",
    "flow_length = 50       #number of planar flow layers K\n",
    "lr = 0.005             #learning rate for the optimizer\n",
    "\n",
    "#set the number of samples in each batch\n",
    "batch_size = 100\n",
    "\n",
    "#set the number of iterations\n",
    "num_iter = 8000\n",
    "  \n",
    "#create the model, optimizer using Adam, target density, and loss function\n",
    "model = PlanarFlow(K=flow_length)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "density = TargetDistribution(target_distr, t=1, a=a)\n",
    "bound = VariationalLoss(density, mean=mean_starting, std=std_starting)\n",
    "\n",
    "#learning rate scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "\n",
    "#start time\n",
    "start = time.default_timer()\n",
    "\n",
    "#optimization using no annealing scheduler\n",
    "#train the model  \n",
    "for iter_num in range(1, num_iter + 1):\n",
    "            \n",
    "    #get the batches from starting distribution\n",
    "    batch = torch.zeros(batch_size).normal_(mean=mean_starting, std=std_starting)\n",
    "\n",
    "    #pass the batch through the planar flow model\n",
    "    zk, log_jacobians = model(batch)\n",
    "\n",
    "    #compute the loss\n",
    "    loss = bound(batch, zk, log_jacobians)\n",
    "\n",
    "    #train the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "            \n",
    "    #apply a learning rate scheduler\n",
    "    #scheduler.step()\n",
    "\n",
    "#compute time\n",
    "end = time.default_timer()\n",
    "opt_time = end - start\n",
    "    \n",
    "#plot approximation\n",
    "plot_comparison(mean=mean_starting, std=std_starting, lim=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
