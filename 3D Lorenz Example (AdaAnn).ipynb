{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions import MultivariateNormal\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from typing import Callable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pathlib import Path\n",
    "import timeit as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions for rk4 to solve lorenz equations\n",
    "\n",
    "def evalRHSvec(s, r, b, x):\n",
    "    return torch.tensor([s*(-x[0] + x[1]), r*x[0] -x[1] -x[0]*x[2], -b*x[2] + x[0]*x[1]])\n",
    "\n",
    "def solveRK4vec(s, r, b, x0, deltaT, totalSteps):\n",
    "    \n",
    "    #create time and solution vectors\n",
    "    time = torch.zeros(totalSteps)\n",
    "    sol  = torch.zeros((totalSteps,len(x0)))\n",
    "    \n",
    "    #set initial condition\n",
    "    sol[0,:] = x0\n",
    "    \n",
    "    #solve at each time step using rk4\n",
    "    for loopA in range(1,totalSteps):\n",
    "        \n",
    "        time[loopA] = time[loopA-1] + deltaT\n",
    "        \n",
    "        k1 = evalRHSvec(s, r, b, sol[loopA-1,:])\n",
    "        k2 = evalRHSvec(s, r, b, sol[loopA-1,:] + 0.5*deltaT*k1)\n",
    "        k3 = evalRHSvec(s, r, b, sol[loopA-1,:] + 0.5*deltaT*k2)\n",
    "        k4 = evalRHSvec(s, r, b, sol[loopA-1,:] + deltaT*k3)\n",
    "        \n",
    "        sol[loopA,:] = sol[loopA-1,:] + (deltaT/6.0)*(k1 + 2*k2 + 2*k3 + k4)\n",
    "        \n",
    "    return sol\n",
    "\n",
    "def solveLorenzVec(s, b, r, num_data):\n",
    "    \n",
    "    #set final time and step size\n",
    "    totalTime = 1.53                      #final time step will be 1.5\n",
    "    deltaT = 0.025\n",
    "    totalSteps = int(totalTime/deltaT)    #60 steps\n",
    "\n",
    "    #set initial condition\n",
    "    x0 = torch.tensor([1.0, 1.0, 1.0])\n",
    "\n",
    "    #solve using rk4\n",
    "    sol = solveRK4vec(s, r, b, x0, deltaT, totalSteps)\n",
    "\n",
    "    #select 30 data points (of the 60) to use\n",
    "    data = torch.zeros(num_data, 3)\n",
    "    index=0\n",
    "    \n",
    "    for j in range(len(sol)):\n",
    "        if j % 2 == 0:\n",
    "            data[index] = sol[j]\n",
    "            index+=1\n",
    "            \n",
    "    return data, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the planar flow class chains the planar flow layers using nn.Sequential\n",
    "\n",
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, K: int = 6): #K is the number of planar flow layers chained together\n",
    "        super().__init__()\n",
    "        \n",
    "        #chain the planar transforms together in a list\n",
    "        self.layers = [PlanarTransform() for _ in range(K)]\n",
    "        \n",
    "        #create the model from the list\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, z: Tensor) -> Tuple[Tensor, float]:\n",
    "        \n",
    "        #set the log of the Jacobian to zero\n",
    "        log_det_J = 0\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            #sum the log Jacobian of each layer\n",
    "            log_det_J += layer.log_det_J(z)\n",
    "            \n",
    "            #calculate the new points from the planar flow\n",
    "            z = layer(z)\n",
    "            \n",
    "        return z, log_det_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the planar transform class contains the functions for the planar flow\n",
    "\n",
    "class PlanarTransform(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #randomly assign starting values to the planar flow parameters from a normal distribution\n",
    "        self.u = nn.Parameter(torch.randn(1, 3).normal_(0, 0.1))\n",
    "        self.w = nn.Parameter(torch.randn(1, 3).normal_(0, 0.1))\n",
    "        self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
    "        \n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #check the invertibility condition\n",
    "        if torch.mm(self.u, self.w.T) < -1:\n",
    "            \n",
    "            #update u to ensure invertibility\n",
    "            self.get_u_hat()\n",
    "            \n",
    "        #return the planar flow layer function\n",
    "        return z + self.u*nn.Tanh()(torch.mm(z, self.w.T) + self.b)\n",
    "    \n",
    "    def log_det_J(self, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #check the invertibility condition\n",
    "        if torch.mm(self.u, self.w.T) < -1:\n",
    "            self.get_u_hat()\n",
    "            \n",
    "        #calculate the log of the Jacobian\n",
    "        a = torch.mm(z, self.w.T) + self.b\n",
    "        psi = (1 - nn.Tanh()(a)**2)*self.w\n",
    "        abs_det = (1 + torch.mm(self.u, psi.T)).abs()\n",
    "        log_det = torch.log(1e-10 + abs_det)\n",
    "        \n",
    "        return log_det\n",
    "    \n",
    "    def get_u_hat(self) -> None:\n",
    "        \n",
    "        #invertibility condition\n",
    "        wtu = torch.mm(self.u, self.w.T)\n",
    "        m_wtu = -1 + torch.log(1 + torch.exp(wtu))\n",
    "        self.u.data = (self. u + (m_wtu - wtu)*self.w/torch.norm(self.w, p=2, dim=1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the target distribution class holds the definitions for the 3D target distributions used in the lorenz example\n",
    "\n",
    "class TargetDistribution:\n",
    "    \n",
    "    def __init__(self, name: str, t: int, sigma_sqr: int):  #t is the annealing value, sigma_sqr is the error used\n",
    "        \n",
    "        #get the name of the target distirbution to be used\n",
    "        self.func = self.get_target_distribution(name, t, sigma_sqr)\n",
    "        \n",
    "    def __call__(self, d:Tensor, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #return the target function\n",
    "        return self.func(d, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_target_distribution(name: str, t: int, sigma_sqr: int) -> Callable[[Tensor], Tensor]:\n",
    "        \n",
    "        if name == \"Lorenz\":\n",
    "            \n",
    "            #target posterior for lorenz equations using a gaussian likelihood and uniform prior\n",
    "            def Lorenz(d, params):\n",
    "                \n",
    "                def evalRHS(s, r, b, x, y, z):\n",
    "                    return s*(-x + y), r*x - y - x*z, -b*z + x*y\n",
    "\n",
    "                def solveRK4 (s, r, b, x0, y0, z0, deltaT, totalSteps, true_data):\n",
    "\n",
    "                    #set starting time to zero\n",
    "                    time = torch.tensor(0.0, dtype=torch.double)\n",
    "\n",
    "                    #set initial conditions\n",
    "                    x = x0\n",
    "                    y = y0\n",
    "                    z = z0\n",
    "                    \n",
    "                    #compute the norm at the initial conditions\n",
    "                    val = (true_data[0][0] - x0)**2 + (true_data[0][1] - y0)**2 + (true_data[0][2] - z0)**2\n",
    "\n",
    "                    #solve at each time step using rk4\n",
    "                    j = 1\n",
    "                    for loopA in range(1, totalSteps):\n",
    "\n",
    "                        time = time + deltaT\n",
    "\n",
    "                        k1x, k1y, k1z = evalRHS(s, r, b, x, y, z)\n",
    "                        k2x, k2y, k2z = evalRHS(s, r, b, x + 0.5*deltaT*k1x, y + 0.5*deltaT*k1y, z + 0.5*deltaT*k1z)\n",
    "                        k3x, k3y, k3z = evalRHS(s, r, b, x + 0.5*deltaT*k2x, y + 0.5*deltaT*k2y, z + 0.5*deltaT*k2z)\n",
    "                        k4x, k4y, k4z = evalRHS(s, r, b, x + deltaT*k3x, y + deltaT*k3y, z + deltaT*k3z)\n",
    "\n",
    "                        x = x + (deltaT/6.0)*(k1x + 2*k2x + 2*k3x + k4x)\n",
    "                        y = y + (deltaT/6.0)*(k1y + 2*k2y + 2*k3y + k4y)\n",
    "                        z = z + (deltaT/6.0)*(k1z + 2*k2z + 2*k3z + k4z)\n",
    "\n",
    "                        #compute the summation in the exponential of the gaussian likelihood\n",
    "                        #summing the norm between the true data and the current values at the 30 selected time steps\n",
    "                        if loopA % 2 == 0:\n",
    "                            val += (true_data[j][0] - x)**2 + (true_data[j][1] - y)**2 + (true_data[j][2] - z)**2\n",
    "                            j += 1\n",
    "\n",
    "                    return val\n",
    "\n",
    "                def solveLorenz(s, b, r, true_data):\n",
    "                    \n",
    "                    #set the final time and step size\n",
    "                    totalTime = torch.tensor(1.53, dtype=torch.double)                   #final time will be 1.5\n",
    "                    deltaT = torch.tensor(0.025, dtype = torch.double)\n",
    "                    totalSteps = torch.tensor(int(totalTime/deltaT), dtype=torch.int)    #60 time steps\n",
    "\n",
    "                    #set the initial conditions\n",
    "                    x0 = torch.tensor(1.0, dtype=torch.double)\n",
    "                    y0 = torch.tensor(1.0, dtype=torch.double)\n",
    "                    z0 = torch.tensor(1.0, dtype=torch.double)\n",
    "\n",
    "                    #compute the summation in the gaussian likelihood\n",
    "                    val = solveRK4(s, r, b, x0, y0, z0, deltaT, totalSteps, true_data)\n",
    "\n",
    "                    #replace infinite values and limit too large of values\n",
    "                    if val > 10000 or torch.isnan(val):\n",
    "                        val = torch.tensor([10000])\n",
    "                        \n",
    "                    return val\n",
    "                \n",
    "                #compute the posterior probabilities for each sampled point of parameters (s,b,r)\n",
    "                f = torch.zeros(len(params))\n",
    "                for j in range(len(params)):\n",
    "                    \n",
    "                    #exponential summation\n",
    "                    val = solveLorenz(params[j][0], params[j][1], params[j][2], d)\n",
    "                    \n",
    "                    #constant value (doesn't affect the free energy bound)\n",
    "                    c = 1.0/np.sqrt((2*np.pi*sigma_sqr)**(3*30))                        #30 data points, 3 dimensions\n",
    "                    \n",
    "                    #compute the log of the posterior\n",
    "                    f[j] = torch.log(torch.tensor(c)) + -t*val/(2*sigma_sqr)\n",
    "                \n",
    "                return f\n",
    "            \n",
    "            return Lorenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free energy loss function used\n",
    "\n",
    "class VariationalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, distribution: TargetDistribution, mean: int, std: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        #the target distribution\n",
    "        self.distr = distribution\n",
    "        \n",
    "        #the starting distirbution\n",
    "        self.base_distr = MultivariateNormal(mean*torch.zeros(3), (std**2)*torch.eye(3))\n",
    "        \n",
    "    def forward(self, d: Tensor, z0: Tensor, z: Tensor, sum_log_det_J: float) -> float:\n",
    "        \n",
    "        #calculate the log of the starting distribution at initial points z0\n",
    "        base_log_prob = self.base_distr.log_prob(z0)\n",
    "        \n",
    "        #calculate the log of the target distribution at final points z\n",
    "        #this function returns the log of the target density\n",
    "        target_density_log_prob = self.distr(d, z)\n",
    "        \n",
    "        #calculate the free energy\n",
    "        return (base_log_prob - target_density_log_prob - sum_log_det_J).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definitions required for plotting purposes\n",
    "\n",
    "#the starting function\n",
    "def StartingFunction(x, mean, std):\n",
    "    return scipy.stats.multivariate_normal.pdf(x, [mean,mean,mean], [[std**2,0,0], [0,std**2,0], [0,0,std**2]])\n",
    "\n",
    "#the jacobian calculation using tanh\n",
    "def Jacobian(x, P):\n",
    "    \n",
    "    num_points = np.shape(x)[0]\n",
    "    J = np.zeros(num_points)\n",
    "    \n",
    "    u = np.array([P[0], P[1], P[2]])\n",
    "    w = np.array([P[3], P[4], P[5]])\n",
    "    b = P[6]\n",
    "    \n",
    "    for j in range(num_points):\n",
    "        J[j] = np.abs(1 + u@w.T*(1 - np.tanh(w.T@x[j] + b)**2))\n",
    "        \n",
    "    return J \n",
    "\n",
    "#compute the inverse\n",
    "def H(x, P):\n",
    "    \n",
    "    u = np.array([P[0], P[1], P[2]])\n",
    "    w = np.array([P[3], P[4], P[5]])\n",
    "    b = P[6]\n",
    "    \n",
    "    X = x + u*np.tanh(w.T@x + b)\n",
    "    return X\n",
    "\n",
    "def computeInverse(z, P):\n",
    "    \n",
    "    u = np.array([P[0], P[1], P[2]])\n",
    "    w = np.array([P[3], P[4], P[5]])\n",
    "    b = P[6]\n",
    "    \n",
    "    hInverse = (z - u*b)/(w + u*w)\n",
    "    num_points = np.shape(hInverse)[0]\n",
    "    \n",
    "    for j in range(num_points):\n",
    "        \n",
    "        def optFun(x):\n",
    "            return H(x, P) - z[j]\n",
    "        \n",
    "        hInverse[j] = scipy.optimize.fsolve(optFun, hInverse[j])\n",
    "        \n",
    "    return hInverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the definitions for plotting\n",
    "\n",
    "plt.rc('font', family='Arial') \n",
    "plt.rc('xtick', labelsize='x-small') \n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "    \n",
    "def plot_optimized(model, mean, std, points=500, cmap=cm.inferno):\n",
    "    \n",
    "    #compute the transformed sample points\n",
    "    z0 = torch.zeros((points,3)).normal_(mean=mean, std=std)\n",
    "    zk, log_jacobians = model(z0)\n",
    "    zk = zk.detach().numpy()\n",
    "    \n",
    "    #compute the probability\n",
    "    prob = np.exp(np.log(StartingFunction(z0)) - log_jacobians.detach().numpy())\n",
    "    \n",
    "    #create the figure and subplot\n",
    "    fig = plt.figure(figsize=(10, 6), dpi=300)\n",
    "    ax1 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    #3d plot\n",
    "    CB = ax1.scatter(zk[:,0], zk[:,1], zk[:,2], c=prob, cmap=cm.inferno, alpha=0.3)\n",
    "    plt.colorbar(CB)\n",
    "    ax1.set_xlabel('s')\n",
    "    ax1.set_ylabel('b')\n",
    "    ax1.set_zlabel('r')\n",
    "    plt.title('Optimized PDF')\n",
    "    #plt.savefig(f'Optimized PDF.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_tvals(tvals):\n",
    "    \n",
    "    #plot\n",
    "    fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "    plt.plot(np.linspace(1,len(tvals), len(tvals)), tvals, 'darkmagenta', lw=1)\n",
    "    plt.title('Annealing Schedule')\n",
    "    #plt.savefig(f'Annealing Schedule.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set true parameter values, number of data points, and error\n",
    "s_true = 10\n",
    "b_true = 8/3\n",
    "r_true = 28\n",
    "num_data = 31\n",
    "sigma_sqr = 0.2\n",
    "\n",
    "#add Gaussian noise to selected data points (not including initial values)\n",
    "data_no_noise, sol = solveLorenzVec(s_true, b_true, r_true, num_data)\n",
    "error = torch.zeros(num_data,3)\n",
    "error[1:num_data] = MultivariateNormal(torch.zeros(3), sigma_sqr*torch.eye(3)).sample((num_data - 1,))\n",
    "data = data_no_noise + error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot trajectory and noisy data\n",
    "time1 = np.linspace(0,1.5,61)\n",
    "time2 = np.linspace(0,1.5,31)\n",
    "\n",
    "fig = plt.figure(figsize=(11, 8), dpi=300)\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax4 = fig.add_subplot(224)\n",
    "\n",
    "#3d trajectory\n",
    "ax1.plot(sol[:,0].detach().numpy(), sol[:,1].detach().numpy(), sol[:,2].detach().numpy(), color='darkmagenta')\n",
    "ax1.plot(data[:,0].detach().numpy(), data[:,1].detach().numpy(), data[:,2].detach().numpy(), '.', color='tomato')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('z')\n",
    "\n",
    "#x component\n",
    "ax2.plot(time1, sol[:,0], color='darkmagenta')\n",
    "ax2.plot(time2, data[:,0], '.', color='tomato')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('x')\n",
    "\n",
    "#y component\n",
    "ax3.plot(time1, sol[:,1], color='darkmagenta')\n",
    "ax3.plot(time2, data[:,1], '.', color='tomato')\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('y')\n",
    "\n",
    "#z component\n",
    "ax4.plot(time1, sol[:,2], color='darkmagenta')\n",
    "ax4.plot(time2, data[:,2], '.', color='tomato')\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_ylabel('z')\n",
    "\n",
    "plt.suptitle('3D and Component Trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaAnn scheduler set-up\n",
    "\n",
    "#list to collect t values\n",
    "tvals = []\n",
    "\n",
    "#choose the starting distribution parameters\n",
    "mean_starting = 10\n",
    "std_starting = 2\n",
    "    \n",
    "#choose the target distribution\n",
    "target_distr = \"Lorenz\"\n",
    "\n",
    "#set the parameters\n",
    "flow_length = 250\n",
    "lr = 0.001\n",
    "t0 = 0.05\n",
    "tol = 0.5\n",
    "M = 100    \n",
    "dt = 0\n",
    "\n",
    "#set the number of samples in each batch\n",
    "N = 100                #at t0 and each annealing step\n",
    "N_1 = 200              #at t = 1\n",
    "\n",
    "#set the number of iterations\n",
    "T_0 = 500              #at t0\n",
    "T = 5                  #at each annealing step\n",
    "T_1 = 5000             #at t = 1\n",
    "\n",
    "#create the model and optimizer using Adam\n",
    "model = PlanarFlow(K=flow_length)    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#start time\n",
    "start = time.default_timer()\n",
    "\n",
    "#optimization using AdaAnn\n",
    "t = t0\n",
    "while t < 1:\n",
    "        \n",
    "    #new t value\n",
    "    t = min(1, t + dt)\n",
    "    tvals = np.concatenate([tvals, np.array([t])])\n",
    "        \n",
    "    #number of iterations and batch size at each annealing step\n",
    "    num_iter = T\n",
    "    batch_size = N\n",
    "    \n",
    "    #update parameters at t0\n",
    "    if t == t0:\n",
    "        num_batches = T_0\n",
    "      \n",
    "    #update parameters at t = 1 and include a learning rate scheduler\n",
    "    if t == 1:\n",
    "        num_iter = T_1\n",
    "        batch_size = N_1\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 500, gamma=0.75)\n",
    "    \n",
    "    #update the target density and loss function with current t value\n",
    "    density = TargetDistribution(target_distr, t=t, sigma_sqr=sigma_sqr)\n",
    "    bound = VariationalLoss(density, mean=mean_starting, std=std_starting) \n",
    "\n",
    "    #train the model\n",
    "    for iter_num in range(1, num_iter + 1):\n",
    "\n",
    "        #get the batches from starting distribution\n",
    "        batch = torch.zeros((batch_size, 3)).normal_(mean=mean_starting, std=std_starting)\n",
    "        \n",
    "        #pass the batch through the planar flow model\n",
    "        zk, log_jacobians = model(batch)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = bound(data, batch, zk, log_jacobians)\n",
    "\n",
    "        #train the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #apply a learning rate scheduler when t = 1\n",
    "        if t == 1:\n",
    "            scheduler.step()\n",
    "                    \n",
    "    #compute the dt value using M points\n",
    "    density_dt = TargetDistribution(target_distr, t=1, sigma_sqr=sigma_sqr)\n",
    "    zk, log_jacobians = model(torch.zeros((M,3)).normal_(mean=mean_starting,std=std_starting))\n",
    "    log_qk = density_dt(data, zk)\n",
    "    dt = tol/torch.sqrt(log_qk.var())\n",
    "    dt = dt.detach().numpy()\n",
    "\n",
    "#compute time\n",
    "end = time.default_timer()\n",
    "opt_time = end - start\n",
    "\n",
    "#plot approximation and annealing schedule\n",
    "plot_optimized(model, mean=mean_starting, std=std_starting)\n",
    "plot_tvals(tvals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
