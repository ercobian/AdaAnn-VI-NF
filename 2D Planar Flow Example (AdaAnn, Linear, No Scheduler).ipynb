{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions import MultivariateNormal\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from typing import Callable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pathlib import Path\n",
    "import timeit as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the planar flow class chains the planar flow layers using nn.Sequential\n",
    "\n",
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, K: int = 6): #K is the number of planar flow layers chained together\n",
    "        super().__init__()\n",
    "        \n",
    "        #chain the planar transforms together in a list\n",
    "        self.layers = [PlanarTransform() for _ in range(K)]\n",
    "        \n",
    "        #create the model from the list\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, z: Tensor) -> Tuple[Tensor, float]:\n",
    "        \n",
    "        #set the log of the Jacobian to zero\n",
    "        log_det_J = 0\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            #sum the log Jacobian of each layer\n",
    "            log_det_J += layer.log_det_J(z)\n",
    "            \n",
    "            #calculate the new points from the planar flow\n",
    "            z = layer(z)\n",
    "            \n",
    "        return z, log_det_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the planar transform class contains the functions for the planar flow\n",
    "\n",
    "class PlanarTransform(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #randomly assign starting values to the planar flow parameters from a normal distribution\n",
    "        self.u = nn.Parameter(torch.randn(1, 2).normal_(0, 0.1))\n",
    "        self.w = nn.Parameter(torch.randn(1, 2).normal_(0, 0.1))\n",
    "        self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
    "        \n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #check the invertibility condition\n",
    "        if torch.mm(self.u, self.w.T) < -1:\n",
    "            \n",
    "            #update u to ensure invertibility\n",
    "            self.get_u_hat()\n",
    "            \n",
    "        #return the planar flow layer function\n",
    "        return z + self.u*nn.Tanh()(torch.mm(z, self.w.T) + self.b)\n",
    "    \n",
    "    def log_det_J(self, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #check the invertibility condition\n",
    "        if torch.mm(self.u, self.w.T) < -1:\n",
    "            self.get_u_hat()\n",
    "            \n",
    "        #calculate the log of the Jacobian\n",
    "        a = torch.mm(z, self.w.T) + self.b\n",
    "        psi = (1 - nn.Tanh()(a)**2)*self.w\n",
    "        abs_det = (1 + torch.mm(self.u, psi.T)).abs()\n",
    "        log_det = torch.log(1e-10 + abs_det)\n",
    "        \n",
    "        return log_det\n",
    "    \n",
    "    def get_u_hat(self) -> None:\n",
    "        \n",
    "        #invertibility condition\n",
    "        wtu = torch.mm(self.u, self.w.T)\n",
    "        m_wtu = -1 + torch.log(1 + torch.exp(wtu))\n",
    "        self.u.data = (self. u + (m_wtu - wtu)*self.w/torch.norm(self.w, p=2, dim=1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the target distribution class holds the definitions for the 2D target distribution used in the examples\n",
    "\n",
    "class TargetDistribution:\n",
    "    \n",
    "    def __init__(self, name: str, t: int, a: int): #t is the annealing value, a is the mean for GMM\n",
    "        \n",
    "        \n",
    "        #get the name of the target distirbution to be used\n",
    "        self.func = self.get_target_distribution(name, t, a)\n",
    "        \n",
    "    def __call__(self, z: Tensor) -> Tensor:\n",
    "        \n",
    "        #return the target function\n",
    "        return self.func(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_target_distribution(name: str, t: int, a: int) -> Callable[[Tensor], Tensor]:\n",
    "        \n",
    "        #target function in 2 dimensions\n",
    "        if name == \"Bimodal2D\":\n",
    "        \n",
    "            def Bimodal2D(z):\n",
    "                \n",
    "                f = 0.5*16/(np.pi)*torch.exp(-16*t*((z[:,0]+1+a)**2 + (z[:,1]-a)**2)) + 0.5*16/(np.pi)*torch.exp(-16*t*((z[:,0]-1-a)**2 + (z[:,1]-a)**2))\n",
    "                return f\n",
    "            \n",
    "            return Bimodal2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free energy loss function used\n",
    "\n",
    "class VariationalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, distribution: TargetDistribution, mean: int, std: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        #the target distribution\n",
    "        self.distr = distribution\n",
    "        \n",
    "        #the starting distirbution\n",
    "        self.base_distr = MultivariateNormal(mean*torch.ones(2), (std**2)*torch.eye(2))\n",
    "        \n",
    "    def forward(self, z0: Tensor, z: Tensor, sum_log_det_J: float) -> float:\n",
    "        \n",
    "        #calculate the log of the starting distribution at initial points z0\n",
    "        base_log_prob = self.base_distr.log_prob(z0)\n",
    "        \n",
    "        #calculate the log of the target distribution at final points z\n",
    "        target_density_log_prob = torch.log(self.distr(z)+1e-20)\n",
    "        \n",
    "        #calculate the free energy\n",
    "        return (base_log_prob - target_density_log_prob - sum_log_det_J).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definitions required for plotting purposes\n",
    "\n",
    "#the starting function\n",
    "def StartingFunction(x, mean, std):\n",
    "    return scipy.stats.multivariate_normal.pdf(x, [mean,mean], [[std**2,0], [0,std**2]])\n",
    "\n",
    "#the jacobian calculation using tanh\n",
    "def Jacobian(x, P):\n",
    "    \n",
    "    num_points = np.shape(x)[0]\n",
    "    J = np.zeros(num_points)\n",
    "    \n",
    "    u = np.array([P[0], P[1]])\n",
    "    w = np.array([P[2], P[3]])\n",
    "    b = P[4]\n",
    "    \n",
    "    for j in range(num_points):\n",
    "        J[j] = np.abs(1 + u@w.T*(1 - np.tanh(w.T@x[j] + b)**2))\n",
    "        \n",
    "    return J \n",
    "\n",
    "#compute the inverse\n",
    "def H(x, P):\n",
    "    \n",
    "    u = np.array([P[0], P[1]])\n",
    "    w = np.array([P[2], P[3]])\n",
    "    b = P[4]\n",
    "    \n",
    "    X = x + u*np.tanh(w.T@x + b)\n",
    "    return X\n",
    "\n",
    "def computeInverse(z, P):\n",
    "    \n",
    "    u = np.array([P[0], P[1]])\n",
    "    w = np.array([P[2], P[3]])\n",
    "    b = P[4]\n",
    "    \n",
    "    hInverse = (z - u*b)/(w + u*w)\n",
    "    num_points = np.shape(hInverse)[0]\n",
    "    \n",
    "    for j in range(num_points):\n",
    "        \n",
    "        def optFun(x):\n",
    "            return H(x, P) - z[j]\n",
    "        \n",
    "        hInverse[j] = scipy.optimize.fsolve(optFun, hInverse[j])\n",
    "        \n",
    "    return hInverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the definitions for plotting\n",
    "\n",
    "plt.rc('font', family='Arial') \n",
    "plt.rc('xtick', labelsize='x-small') \n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "\n",
    "def plot_points(num, lim=5):\n",
    "    \n",
    "    #create the points in x and y directions for plotting\n",
    "    x = y = torch.linspace(-lim, lim, num)\n",
    "        \n",
    "    #create the mesh grid\n",
    "    X, Y = torch.meshgrid(x, y)\n",
    "        \n",
    "    #reshape the mesh into a vector\n",
    "    shape = X.shape\n",
    "    X_flatten, Y_flatten = np.reshape(X, (-1, 1)), np.reshape(Y, (-1, 1))\n",
    "    XY = torch.from_numpy(np.concatenate([X_flatten, Y_flatten], 1))\n",
    "        \n",
    "    return X, Y, XY, shape\n",
    "    \n",
    "def plot_density(density, num=500, lim=5, cmap=cm.magma):\n",
    "        \n",
    "    X, Y, XY, shape = plot_points(num, lim)\n",
    "\n",
    "    #calculate the density function\n",
    "    Z = density(XY)\n",
    "\n",
    "    #reshape the vector into a grid\n",
    "    Z = Z.reshape(shape)\n",
    "    Z = Z.detach().numpy()\n",
    "\n",
    "    #create the figure and subplots\n",
    "    fig = plt.figure(figsize=(11, 4), dpi=300)\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    #3d plot\n",
    "    ax1.plot_surface(X, Y, Z, cmap=cmap, antialiased=True)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    \n",
    "    #2d plot\n",
    "    twoD = ax2.pcolormesh(X, Y, Z, cmap=cmap, shading='auto')\n",
    "    plt.colorbar(twoD)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "\n",
    "    plt.suptitle('Target PDF')\n",
    "    #plt.savefig(f'2D Target PDF.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def compute_opt(mean, std, num, lim):\n",
    "    \n",
    "    #put the parameters into a vector\n",
    "    P = []\n",
    "    for param in model.parameters():\n",
    "        p = param.detach().numpy().tolist()\n",
    "        p = np.reshape(p, -1)\n",
    "        P = np.concatenate([P, p])\n",
    "            \n",
    "    #points\n",
    "    X, Y, XY, shape = plot_points(num, lim)\n",
    "    XY = XY.detach().numpy().tolist()\n",
    "    XY = np.array(XY)\n",
    "        \n",
    "    #compute the optimized pdf\n",
    "    Z = np.ones(np.shape(XY)[0])\n",
    "    \n",
    "    i = 5*(flow_length - 1)\n",
    "    for j in range(flow_length):\n",
    "        XY = computeInverse(XY, P[i:i+5])\n",
    "        Z = Z/Jacobian(XY, P[i:i+5])\n",
    "        i-=5\n",
    "            \n",
    "    Z = Z*StartingFunction(XY, mean, std)\n",
    "    Z = Z.reshape(num, num)\n",
    "    \n",
    "    return X, Y, Z\n",
    "    \n",
    "#two different ways to plot the optimized pdf (using a grid vs sample points)\n",
    "def plot_optimized(mean, std, num=100, lim=5, cmap=cm.magma):\n",
    "    \n",
    "    #compute the optimzed pdf\n",
    "    X, Y, Z = compute_opt(mean, std, num, lim)\n",
    "    \n",
    "    #create the figure and subplots\n",
    "    fig = plt.figure(figsize=(11, 4), dpi=300)\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    #3d plot\n",
    "    ax1.plot_surface(X, Y, Z, cmap=cmap, antialiased=True)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    \n",
    "    #2d plot\n",
    "    twoD = ax2.pcolormesh(X, Y, Z, cmap=cmap, shading='auto')\n",
    "    plt.colorbar(twoD)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    \n",
    "    plt.suptitle('Optimized PDF')\n",
    "    #plt.savefig(f'2D Optimized PDF.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_model_points(model, mean, std, num=500, lim=5, cmap=cm.magma):\n",
    "    \n",
    "    #get points\n",
    "    _, _, XY, _ = plot_points(num, 5*lim)\n",
    "    \n",
    "    #compute optimized points\n",
    "    Z, sum_log_jacobians = model(XY)\n",
    "    base_distr = MultivariateNormal(mean*torch.ones(2), (std**2)*torch.eye(2))\n",
    "    base_log_prob = base_distr.log_prob(XY).reshape(1,num**2)\n",
    "    final_log_prob = base_log_prob - sum_log_jacobians\n",
    "    prob = torch.exp(final_log_prob)\n",
    "    \n",
    "    #reshape points\n",
    "    X = Z[:, 0].detach().reshape(num, num)\n",
    "    Y = Z[:, 1].detach().reshape(num, num)\n",
    "    prob = prob.detach().reshape(num, num)\n",
    "    \n",
    "    #plot\n",
    "    fig = plt.figure(figsize=(4, 3), dpi=300)\n",
    "    twoD = plt.pcolormesh(X, Y, prob, cmap=cmap, shading='auto')\n",
    "    plt.colorbar(twoD)\n",
    "    plt.xlim(-lim, lim)\n",
    "    plt.ylim(-lim, lim)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    \n",
    "    plt.title('Optimized PDF')\n",
    "    #plt.savefig(f'2D Optimized PDF.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_tvals(tvals):\n",
    "    \n",
    "    #plot\n",
    "    fig = plt.figure(figsize=(6, 4), dpi=300)\n",
    "    plt.plot(np.linspace(1,len(tvals), len(tvals)), tvals, 'darkmagenta', lw=1)\n",
    "    plt.title('Annealing Schedule')\n",
    "    #plt.savefig(f'Annealing Schedule.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaAnn scheduler set-up\n",
    "    \n",
    "#list to collect t values\n",
    "tvals = []\n",
    "\n",
    "#choose the starting distribution parameters\n",
    "mean_starting = 0\n",
    "std_starting = 2\n",
    "    \n",
    "#choose the target distribution and set a\n",
    "target_distr = \"Bimodal2D\"\n",
    "a = 0\n",
    "\n",
    "#plot target density\n",
    "density_target = TargetDistribution(target_distr, t=1, a=a)\n",
    "plot_density(density_target, lim=4)\n",
    "\n",
    "#set the parameters\n",
    "flow_length = 75       #number of planar flow layers K\n",
    "lr = 0.0005            #learning rate for the optimizer\n",
    "t0 = 0.01              #starting t value\n",
    "tol = 0.01             #KL divergence tolerance for AdaAnn\n",
    "M = 1000               #number of sample points to compute step size\n",
    "dt = 0\n",
    "\n",
    "#set the number of samples in each batch\n",
    "N = 100                #at t0 and each annealing step\n",
    "N_1 = 1000             #at t = 1\n",
    "\n",
    "#set the number of iterations\n",
    "T_0 = 500              #at t0\n",
    "T = 5                  #at each annealing step\n",
    "T_1 = 5000             #at t = 1\n",
    "\n",
    "#create the model and optimizer using Adam\n",
    "model = PlanarFlow(K=flow_length)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "#start time\n",
    "start = time.default_timer()\n",
    "\n",
    "#optimization using AdaAnn\n",
    "t = t0\n",
    "while t < 1:\n",
    "    \n",
    "    #new t value        \n",
    "    t = min(1, t + dt) \n",
    "    tvals = np.concatenate([tvals, np.array([t])])\n",
    "      \n",
    "    #number of iterations and batch size at each annealing step\n",
    "    num_iter = T\n",
    "    batch_size = N\n",
    "        \n",
    "    #update parameters at t0\n",
    "    if t == t0:\n",
    "        num_batches = T_0\n",
    "     \n",
    "    #update parameters at t = 1 and include a learning rate scheduler\n",
    "    if t == 1:\n",
    "        num_iter = T_1\n",
    "        batch_size = N_1\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.75)\n",
    "            \n",
    "    #update the target density and loss function with current t value\n",
    "    density = TargetDistribution(target_distr, t=t, a=a)\n",
    "    bound = VariationalLoss(density, mean=mean_starting, std=std_starting)\n",
    "        \n",
    "    #train the model   \n",
    "    for iter_num in range(1, num_iter + 1):\n",
    "            \n",
    "        #get the batches from starting distribution\n",
    "        batch = torch.zeros((batch_size,2)).normal_(mean=mean_starting, std=std_starting)\n",
    "\n",
    "        #pass the batch through the planar flow model\n",
    "        zk, log_jacobians = model(batch)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = bound(batch, zk, log_jacobians)\n",
    "\n",
    "        #train the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "        #apply a learning rate scheduler when t = 1\n",
    "        if t == 1:\n",
    "            scheduler.step()\n",
    "       \n",
    "    #compute the dt value using M points\n",
    "    density_dt = TargetDistribution(target_distr, t=1, a=a)\n",
    "    zk, log_jacobians = model(torch.zeros((M,2)).normal_(mean=mean_starting, std=std_starting))\n",
    "    log_qk = torch.log(density_dt(zk) + 1e-10)\n",
    "    dt = tol/torch.sqrt(log_qk.var())\n",
    "    dt = dt.detach().numpy()\n",
    "    \n",
    "#compute time\n",
    "end = time.default_timer()\n",
    "opt_time = end - start\n",
    "    \n",
    "#plot approximation and annealing schedule\n",
    "plot_model_points(model, mean=mean_starting, std=std_starting)\n",
    "plot_tvals(tvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear scheduler set-up\n",
    "    \n",
    "#list to collect t values\n",
    "tvals = []\n",
    "\n",
    "#choose the starting distribution parameters\n",
    "mean_starting = 0\n",
    "std_starting = 2\n",
    "    \n",
    "#choose the target distribution and set a\n",
    "target_distr = \"Bimodal2D\"\n",
    "a = 0\n",
    "\n",
    "#plot target density\n",
    "density_target = TargetDistribution(target_distr, t=1, a=a)\n",
    "plot_density(density_target, lim=4)\n",
    "\n",
    "#set the parameters\n",
    "flow_length = 75       #number of planar flow layers K\n",
    "lr = 0.0005            #learning rate for the optimizer\n",
    "t0 = 0.01              #starting t value\n",
    "eps = 1/10000          #set constant step size\n",
    "dt = 0\n",
    "\n",
    "#set the number of samples in each batch\n",
    "N = 100                #at t0 and each annealing step\n",
    "N_1 = 1000             #at t = 1\n",
    "\n",
    "#set the number of iterations\n",
    "T_0 = 500              #at t0\n",
    "T = 1                  #at each annealing step\n",
    "T_1 = 5000             #at t = 1\n",
    "\n",
    "#create the model and optimizer using Adam\n",
    "model = PlanarFlow(K=flow_length)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "#start time\n",
    "start = time.default_timer()\n",
    "\n",
    "#optimization using AdaAnn\n",
    "t = t0\n",
    "while t < 1:\n",
    "    \n",
    "    #new t value        \n",
    "    t = min(1, t + dt) \n",
    "    tvals = np.concatenate([tvals, np.array([t])])\n",
    "      \n",
    "    #number of iterations and batch size at each annealing step\n",
    "    num_iter = T\n",
    "    batch_size = N\n",
    "        \n",
    "    #update parameters at t0\n",
    "    if t == t0:\n",
    "        num_batches = T_0\n",
    "     \n",
    "    #update parameters at t = 1 and include a learning rate scheduler\n",
    "    if t == 1:\n",
    "        num_iter = T_1\n",
    "        batch_size = N_1\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.75)\n",
    "            \n",
    "    #update the target density and loss function with current t value\n",
    "    density = TargetDistribution(target_distr, t=t, a=a)\n",
    "    bound = VariationalLoss(density, mean=mean_starting, std=std_starting)\n",
    "        \n",
    "    #train the model   \n",
    "    for iter_num in range(1, num_iter + 1):\n",
    "            \n",
    "        #get the batches from starting distribution\n",
    "        batch = torch.zeros((batch_size,2)).normal_(mean=mean_starting, std=std_starting)\n",
    "\n",
    "        #pass the batch through the planar flow model\n",
    "        zk, log_jacobians = model(batch)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = bound(batch, zk, log_jacobians)\n",
    "\n",
    "        #train the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "        #apply a learning rate scheduler when t = 1\n",
    "        if t == 1:\n",
    "            scheduler.step()\n",
    "       \n",
    "    dt = eps\n",
    "    \n",
    "#compute time\n",
    "end = time.default_timer()\n",
    "opt_time = end - start\n",
    "    \n",
    "#plot approximation and annealing schedule\n",
    "plot_model_points(model, mean=mean_starting, std=std_starting)\n",
    "plot_tvals(tvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no scheduler set-up\n",
    "    \n",
    "#choose the target distribution and set a\n",
    "target_distr = \"Bimodal2D\"\n",
    "a = 0\n",
    "\n",
    "#choose the starting distribution parameters\n",
    "mean_starting = 0\n",
    "std_starting = 2\n",
    "\n",
    "#plot target density\n",
    "density_target = TargetDistribution(target_distr, t=1, a=a)\n",
    "plot_density(density_target, lim=4)\n",
    "\n",
    "#set the parameters\n",
    "flow_length = 75       #number of planar flow layers K\n",
    "lr = 0.0005            #learning rate for the optimizer\n",
    "\n",
    "#set the number of samples in each batch\n",
    "batch_size = 100\n",
    "\n",
    "#set the number of iterations\n",
    "num_iter = 5000\n",
    "\n",
    "#create the model, optimizer using Adam, target density, and loss function\n",
    "model = PlanarFlow(K=flow_length)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "density = TargetDistribution(target_distr, t=t, a=a)\n",
    "bound = VariationalLoss(density, mean=mean_starting, std=std_starting)\n",
    "    \n",
    "#learning rate scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.75)       \n",
    "\n",
    "#start time\n",
    "start = time.default_timer()\n",
    "\n",
    "#optimization using no annealing scheduler\n",
    "#train the model   \n",
    "for iter_num in range(1, num_iter + 1):\n",
    "            \n",
    "    #get the batches from starting distribution\n",
    "    batch = torch.zeros((batch_size,2)).normal_(mean=mean_starting, std=std_starting)\n",
    "\n",
    "    #pass the batch through the planar flow model\n",
    "    zk, log_jacobians = model(batch)\n",
    "\n",
    "    #compute the loss\n",
    "    loss = bound(batch, zk, log_jacobians)\n",
    "\n",
    "    #train the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "         \n",
    "    #apply a learning rate scheduler\n",
    "    #scheduler.step()\n",
    "    \n",
    "#compute time\n",
    "end = time.default_timer()\n",
    "opt_time = end - start\n",
    "    \n",
    "#plot approximation\n",
    "plot_model_points(model, mean=mean_starting, std=std_starting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
